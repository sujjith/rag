# src/rag/models.py - Data models for RAG application
from dataclasses import dataclass, field  # dataclass = auto-generate class boilerplate
from typing import Optional  # Optional = value can be None
from datetime import datetime  # For timestamps

@dataclass
class Document:  # Represents a source document (PDF, TXT, DOCX) loaded into the system
    id: str  # Unique identifier (e.g., UUID or filename hash)
    content: str  # Full text content extracted from the document
    metadata: dict = field(default_factory=dict)  # Extra info (author, page count, etc.) - default_factory avoids mutable default bug
    source: str = ""  # Original file path or URL
    created_at: datetime = field(default_factory=datetime.now)  # When document was added

@dataclass
class Chunk:  # A small piece of a Document (LLMs have token limits, smaller chunks = better search)
    id: str  # Unique identifier for this chunk
    document_id: str  # ID of parent Document (trace back to original source)
    content: str  # Actual text content of this chunk
    embedding: Optional[list[float]] = None  # Vector representation [0.123, -0.456, ...] - generated after chunking
    metadata: dict = field(default_factory=dict)  # Extra info (page number, section title)
    start_char: int = 0  # Position in original document where chunk starts
    end_char: int = 0  # Position in original document where chunk ends

@dataclass
class SearchResult:  # Result from vector similarity search
    chunk: Chunk  # The chunk that matched the search query
    score: float  # Similarity score (0.0 to 1.0, higher = more relevant, based on cosine similarity)

@dataclass
class RAGResponse:  # Final output of RAG system - answer + sources
    answer: str  # The answer generated by the LLM
    sources: list[SearchResult]  # Search results used as context (for citations/transparency)
    query: str  # The original question that was asked