{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.4: Understanding Model Flavors in MLflow\n",
    "\n",
    "This comprehensive notebook demonstrates:\n",
    "1. **What are Flavors** - Understanding the concept\n",
    "2. **Sklearn Flavor** - Logging different sklearn models\n",
    "3. **Pipeline Flavor** - Logging sklearn pipelines\n",
    "4. **Custom PyFunc** - Creating your own model wrapper\n",
    "5. **Loading & Using** - How to load models with different interfaces\n",
    "\n",
    "## What are Model Flavors?\n",
    "\n",
    "**Flavors** are MLflow's way of supporting different ML frameworks. Think of a flavor as a \"format\" or \"packaging\" for your model.\n",
    "\n",
    "### Why Flavors Exist\n",
    "\n",
    "Different ML frameworks save models differently:\n",
    "- **Scikit-learn** uses pickle/joblib\n",
    "- **TensorFlow** uses SavedModel format\n",
    "- **PyTorch** uses .pt/.pth files\n",
    "- **XGBoost** uses its own binary format\n",
    "\n",
    "MLflow flavors provide:\n",
    "- **Native interface**: Use the original framework's API\n",
    "- **Unified interface**: Use `mlflow.pyfunc` for any model type\n",
    "\n",
    "## Common MLflow Flavors\n",
    "\n",
    "| Flavor | Framework | Import |\n",
    "|--------|-----------|--------|\n",
    "| `sklearn` | Scikit-learn | `mlflow.sklearn` |\n",
    "| `pytorch` | PyTorch | `mlflow.pytorch` |\n",
    "| `tensorflow` | TensorFlow | `mlflow.tensorflow` |\n",
    "| `xgboost` | XGBoost | `mlflow.xgboost` |\n",
    "| `lightgbm` | LightGBM | `mlflow.lightgbm` |\n",
    "| `pyfunc` | Any Python | `mlflow.pyfunc` |\n",
    "\n",
    "## Learning Goals\n",
    "- Understand what flavors are and why they're useful\n",
    "- Log different types of sklearn models\n",
    "- Create and log sklearn pipelines\n",
    "- Build a custom PyFunc model\n",
    "- Load models using different interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We'll import MLflow along with various sklearn models and pipeline tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# sklearn datasets and data splitting\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Various sklearn classifiers to compare\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# sklearn pipeline and preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# System\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to learn about model flavors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Connect to MLflow\n",
    "\n",
    "Set up our connection to the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MLflow tracking server URL\n",
    "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n",
    "\n",
    "# Connect to MLflow\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "# Set experiment for this tutorial\n",
    "mlflow.set_experiment(\"phase2-model-flavors\")\n",
    "\n",
    "print(f\"Connected to MLflow at: {TRACKING_URI}\")\n",
    "print(f\"Experiment: phase2-model-flavors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data\n",
    "\n",
    "Load and prepare the Iris dataset for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create DataFrame with feature names\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Features: {list(iris.feature_names)}\")\n",
    "print(f\"Classes: {list(iris.target_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sklearn Flavor - Multiple Model Types\n",
    "\n",
    "The `sklearn` flavor works with ANY scikit-learn model. Let's train and log several different types of classifiers to see how the same flavor handles different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Part 1: Logging Multiple Sklearn Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define several different classifiers to compare\n",
    "# Each uses different algorithms but all can be logged with sklearn flavor\n",
    "models = {\n",
    "    \"random_forest\": RandomForestClassifier(\n",
    "        n_estimators=50,    # 50 decision trees\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=50,    # 50 boosting stages\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"logistic_regression\": LogisticRegression(\n",
    "        max_iter=200,       # Maximum iterations for convergence\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"\\nTraining and logging models:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Train and log each model\n",
    "for name, model in models.items():\n",
    "    # Each run is named after the model type\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Train the model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Create signature from the data\n",
    "        signature = mlflow.models.infer_signature(X, model.predict(X))\n",
    "        \n",
    "        # Log the model using sklearn flavor\n",
    "        # This works for RandomForest, GradientBoosting, LogisticRegression, etc.\n",
    "        mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "        \n",
    "        # Add tags to identify the model\n",
    "        # type(model).__name__ gets the class name like \"RandomForestClassifier\"\n",
    "        mlflow.set_tag(\"model_type\", type(model).__name__)\n",
    "        mlflow.set_tag(\"flavor\", \"sklearn\")\n",
    "        \n",
    "        # Calculate and log training accuracy\n",
    "        accuracy = (model.predict(X) == y).mean()\n",
    "        mlflow.log_metric(\"train_accuracy\", accuracy)\n",
    "        \n",
    "        print(f\"  {name}: accuracy = {accuracy:.4f} ({type(model).__name__})\")\n",
    "\n",
    "print(\"\\nAll sklearn models logged with the same flavor!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sklearn Pipeline\n",
    "\n",
    "A **Pipeline** chains multiple steps together (preprocessing + model). The sklearn flavor can log entire pipelines as a single model!\n",
    "\n",
    "**Benefits of Pipelines:**\n",
    "- All preprocessing is bundled with the model\n",
    "- Prevents data leakage during cross-validation\n",
    "- Easy to deploy - single object handles everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 2: Logging Sklearn Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a pipeline with preprocessing and classification\n",
    "# The pipeline has two steps:\n",
    "# 1. StandardScaler: Normalizes features (mean=0, std=1)\n",
    "# 2. RandomForestClassifier: Makes predictions\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),  # Step 1: Normalize data\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=50, random_state=42))  # Step 2: Classify\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline structure:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (name, step) in enumerate(pipeline.steps, 1):\n",
    "    print(f\"  Step {i}: {name} -> {type(step).__name__}\")\n",
    "\n",
    "# Train the pipeline\n",
    "print(\"\\nTraining pipeline...\")\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Log the pipeline\n",
    "with mlflow.start_run(run_name=\"sklearn-pipeline\"):\n",
    "    # Create signature\n",
    "    signature = mlflow.models.infer_signature(X, pipeline.predict(X))\n",
    "    \n",
    "    # Log the entire pipeline as a single model\n",
    "    # When loaded, it will automatically apply scaling before prediction\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\", signature=signature)\n",
    "    \n",
    "    # Add tags\n",
    "    mlflow.set_tag(\"model_type\", \"Pipeline\")\n",
    "    mlflow.set_tag(\"steps\", \"StandardScaler + RandomForestClassifier\")\n",
    "    \n",
    "    # Log accuracy\n",
    "    accuracy = (pipeline.predict(X) == y).mean()\n",
    "    mlflow.log_metric(\"train_accuracy\", accuracy)\n",
    "    \n",
    "    print(f\"\\nPipeline logged!\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(\"  The model includes both preprocessing AND classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Custom PyFunc Model\n",
    "\n",
    "**PyFunc** (Python Function) is MLflow's universal model format. It allows you to create custom models with:\n",
    "- Custom preprocessing logic\n",
    "- Custom postprocessing (e.g., return class names instead of numbers)\n",
    "- Combine multiple models\n",
    "- Any Python code you need!\n",
    "\n",
    "To create a PyFunc model, inherit from `mlflow.pyfunc.PythonModel` and implement:\n",
    "- `__init__()`: Store your model and configuration\n",
    "- `predict(context, model_input)`: Define prediction logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 3: Creating Custom PyFunc Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define a custom model class\n",
    "# This wraps a sklearn classifier with custom preprocessing and postprocessing\n",
    "\n",
    "class PreprocessingModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A custom model that includes:\n",
    "    - Data normalization (preprocessing)\n",
    "    - Classification\n",
    "    - Returns class names instead of numbers (postprocessing)\n",
    "    \n",
    "    This demonstrates how to wrap existing models with custom logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifier, class_names):\n",
    "        \"\"\"\n",
    "        Initialize the custom model.\n",
    "        \n",
    "        Args:\n",
    "            classifier: Any sklearn classifier\n",
    "            class_names: List of human-readable class names\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        self.class_names = class_names\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit both the scaler and classifier.\n",
    "        \n",
    "        This method trains:\n",
    "        1. The scaler to learn mean/std from training data\n",
    "        2. The classifier on the scaled data\n",
    "        \"\"\"\n",
    "        # First, fit the scaler and transform the data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Then, train the classifier on scaled data\n",
    "        self.classifier.fit(X_scaled, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Make predictions with preprocessing and postprocessing.\n",
    "        \n",
    "        This method:\n",
    "        1. Converts input to numpy array\n",
    "        2. Scales the input (preprocessing)\n",
    "        3. Makes predictions\n",
    "        4. Converts numeric predictions to class names (postprocessing)\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context (can contain model artifacts)\n",
    "            model_input: Input data (DataFrame or array)\n",
    "        \n",
    "        Returns:\n",
    "            List of class names (e.g., ['setosa', 'versicolor', ...])\n",
    "        \"\"\"\n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            X = model_input.values\n",
    "        else:\n",
    "            X = np.array(model_input)\n",
    "        \n",
    "        # Preprocess: Scale the input data\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Make predictions (returns numbers like 0, 1, 2)\n",
    "        predictions = self.classifier.predict(X_scaled)\n",
    "        \n",
    "        # Postprocess: Convert numbers to class names\n",
    "        return [self.class_names[p] for p in predictions]\n",
    "\n",
    "\n",
    "print(\"\\nCustom model class defined!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  - Automatic data scaling\")\n",
    "print(\"  - Returns class NAMES instead of numbers\")\n",
    "print(\"  - Works with any sklearn classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the custom model\n",
    "print(\"\\nCreating and training custom model...\")\n",
    "\n",
    "# Create a RandomForest as the base classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Wrap it in our custom model with Iris class names\n",
    "custom_model = PreprocessingModel(\n",
    "    classifier=rf,\n",
    "    class_names=list(iris.target_names)  # ['setosa', 'versicolor', 'virginica']\n",
    ")\n",
    "\n",
    "# Train the custom model\n",
    "custom_model.fit(X, y)\n",
    "\n",
    "# Test it\n",
    "sample_predictions = custom_model.predict(None, X.iloc[:5])\n",
    "print(f\"\\nSample predictions: {sample_predictions}\")\n",
    "print(\"Note: Returns class NAMES, not numbers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the custom model to MLflow\n",
    "print(\"\\nLogging custom PyFunc model...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"custom-pyfunc\") as pyfunc_run:\n",
    "    # Log using pyfunc flavor (not sklearn!)\n",
    "    # python_model= tells MLflow this is a custom Python class\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=custom_model,\n",
    "        signature=mlflow.models.infer_signature(\n",
    "            X,\n",
    "            custom_model.predict(None, X)  # Sample output for signature\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add tags\n",
    "    mlflow.set_tag(\"model_type\", \"CustomPyFunc\")\n",
    "    mlflow.set_tag(\"flavor\", \"pyfunc\")\n",
    "    mlflow.set_tag(\"features\", \"preprocessing + class_names\")\n",
    "    \n",
    "    saved_pyfunc_run_id = pyfunc_run.info.run_id\n",
    "    \n",
    "    print(f\"\\nCustom PyFunc model logged!\")\n",
    "    print(f\"Run ID: {saved_pyfunc_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loading and Testing Models\n",
    "\n",
    "Now let's load the models we logged and test them. We'll use the `pyfunc` interface which works for ANY flavor - this is the recommended way to load models for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 4: Loading and Testing Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all runs from our experiment\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[\"phase2-model-flavors\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "# Prepare sample input\n",
    "sample_input = X.iloc[:3]\n",
    "\n",
    "print(\"\\nSample input data:\")\n",
    "print(sample_input)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Load and test each model\n",
    "for _, run in runs.iterrows():\n",
    "    run_name = run[\"tags.mlflow.runName\"]\n",
    "    run_id = run[\"run_id\"]\n",
    "    \n",
    "    print(f\"\\n{run_name}:\")\n",
    "    \n",
    "    # Load using pyfunc interface (works for all flavors!)\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    \n",
    "    try:\n",
    "        loaded = mlflow.pyfunc.load_model(model_uri)\n",
    "        preds = loaded.predict(sample_input)\n",
    "        \n",
    "        # Format output based on prediction type\n",
    "        if isinstance(preds[0], str):\n",
    "            # Custom model returns strings\n",
    "            print(f\"  Predictions: {list(preds[:3])}\")\n",
    "        else:\n",
    "            # Standard models return numbers\n",
    "            pred_names = [iris.target_names[p] for p in preds[:3]]\n",
    "            print(f\"  Predictions: {list(preds[:3])} -> {pred_names}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing Sklearn and PyFunc Loading\n",
    "\n",
    "Let's compare the two ways to load a model:\n",
    "1. **Native flavor** (`mlflow.sklearn.load_model`): Returns original sklearn object\n",
    "2. **PyFunc** (`mlflow.pyfunc.load_model`): Returns MLflow wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 5: Sklearn vs PyFunc Loading\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find the random_forest run\n",
    "rf_run = runs[runs[\"tags.mlflow.runName\"] == \"random_forest\"].iloc[0]\n",
    "rf_uri = f\"runs:/{rf_run['run_id']}/model\"\n",
    "\n",
    "print(f\"\\nLoading Random Forest model...\")\n",
    "\n",
    "# Method 1: Load with sklearn flavor\n",
    "print(\"\\n[Method 1: mlflow.sklearn.load_model]\")\n",
    "print(\"-\" * 40)\n",
    "sklearn_model = mlflow.sklearn.load_model(rf_uri)\n",
    "print(f\"Type: {type(sklearn_model).__name__}\")\n",
    "print(f\"n_estimators: {sklearn_model.n_estimators}\")\n",
    "print(f\"Has feature_importances_: {hasattr(sklearn_model, 'feature_importances_')}\")\n",
    "\n",
    "# Method 2: Load with pyfunc\n",
    "print(\"\\n[Method 2: mlflow.pyfunc.load_model]\")\n",
    "print(\"-\" * 40)\n",
    "pyfunc_model = mlflow.pyfunc.load_model(rf_uri)\n",
    "print(f\"Type: {type(pyfunc_model).__name__}\")\n",
    "print(f\"Has feature_importances_: {hasattr(pyfunc_model, 'feature_importances_')}\")\n",
    "\n",
    "# Both make the same predictions\n",
    "print(\"\\n[Comparison]\")\n",
    "print(\"-\" * 40)\n",
    "sklearn_preds = sklearn_model.predict(sample_input)\n",
    "pyfunc_preds = pyfunc_model.predict(sample_input)\n",
    "print(f\"Sklearn predictions: {list(sklearn_preds)}\")\n",
    "print(f\"PyFunc predictions:  {list(pyfunc_preds)}\")\n",
    "print(f\"Same results: {list(sklearn_preds) == list(pyfunc_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Model Flavors\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Approach | When to Use |\n",
    "|----------|-------------|\n",
    "| **Native Flavor** (e.g., `mlflow.sklearn`) | Need access to framework-specific features (like `feature_importances_`) |\n",
    "| **PyFunc Loading** | Deployment, serving, or when you just need predictions |\n",
    "| **Custom PyFunc** | Need custom preprocessing, postprocessing, or logic |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Flavors are Format Handlers**\n",
    "   - Each framework has its own flavor\n",
    "   - MLflow handles serialization/deserialization\n",
    "\n",
    "2. **PyFunc is Universal**\n",
    "   - Works with any flavor\n",
    "   - Provides consistent `predict()` interface\n",
    "   - Best for deployment\n",
    "\n",
    "3. **Custom PyFunc for Custom Logic**\n",
    "   - Inherit from `mlflow.pyfunc.PythonModel`\n",
    "   - Implement `predict(context, model_input)`\n",
    "   - Bundle preprocessing/postprocessing with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Model Flavors Tutorial Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nView at: {TRACKING_URI}\")\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"  1. What model flavors are and why they exist\")\n",
    "print(\"  2. How to log different sklearn model types\")\n",
    "print(\"  3. How to log sklearn pipelines\")\n",
    "print(\"  4. How to create and log custom PyFunc models\")\n",
    "print(\"  5. Difference between native and pyfunc loading\")\n",
    "print(\"\\nTry viewing the models in MLflow UI and comparing their structures!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
