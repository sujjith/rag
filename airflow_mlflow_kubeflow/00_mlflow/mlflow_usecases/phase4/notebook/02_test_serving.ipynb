{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.2: Testing Model Serving Endpoint\n",
    "\n",
    "This comprehensive notebook demonstrates:\n",
    "1. **Making REST API Predictions** - Call the model server\n",
    "2. **Different Input Formats** - inputs, dataframe_split, dataframe_records\n",
    "3. **Batch Predictions** - Process multiple samples\n",
    "4. **Error Handling** - Handle common errors\n",
    "\n",
    "## Prerequisites\n",
    "**Important:** \n",
    "1. Run `01_prepare_model.ipynb` first to prepare the model\n",
    "2. Start the model server before running this notebook:\n",
    "   ```bash\n",
    "   mlflow models serve -m 'models:/iris-serving-model/Production' -p 5001 --no-conda\n",
    "   ```\n",
    "\n",
    "## MLflow Serving Architecture\n",
    "\n",
    "```\n",
    "┌──────────────────┐     HTTP/JSON     ┌──────────────────┐\n",
    "│   Your Client    │─────────────────>│  MLflow Server   │\n",
    "│   (Python, JS,   │                  │                  │\n",
    "│    curl, etc.)   │<─────────────────│  Loads model     │\n",
    "│                  │    Predictions    │  from Registry   │\n",
    "└──────────────────┘                  └──────────────────┘\n",
    "```\n",
    "\n",
    "## Learning Goals\n",
    "- Make predictions via REST API\n",
    "- Understand different input formats\n",
    "- Handle batch predictions\n",
    "- Debug common errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests: For making HTTP calls to the model server\n",
    "import requests\n",
    "\n",
    "# json: For formatting request/response data\n",
    "import json\n",
    "\n",
    "# pandas & numpy: For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn: For test data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# time: For performance testing\n",
    "import time\n",
    "\n",
    "# collections: For result analysis\n",
    "from collections import Counter\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to test model serving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model server configuration\n",
    "MODEL_SERVER = \"http://localhost:5001\"\n",
    "ENDPOINT = f\"{MODEL_SERVER}/invocations\"  # Prediction endpoint\n",
    "\n",
    "# Load Iris metadata for interpreting results\n",
    "iris = load_iris()\n",
    "feature_names = list(iris.feature_names)\n",
    "target_names = list(iris.target_names)\n",
    "\n",
    "print(f\"Model server: {MODEL_SERVER}\")\n",
    "print(f\"Prediction endpoint: {ENDPOINT}\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "print(f\"Classes: {target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_server():\n",
    "    \"\"\"\n",
    "    Check if the model server is running.\n",
    "    Returns True if server is up, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to hit the health endpoint\n",
    "        response = requests.get(f\"{MODEL_SERVER}/health\", timeout=5)\n",
    "        return response.ok\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def predict_inputs(data):\n",
    "    \"\"\"\n",
    "    Make predictions using the 'inputs' format.\n",
    "    This is the simplest format - just a list of lists.\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists, e.g., [[5.1, 3.5, 1.4, 0.2]]\n",
    "    \n",
    "    Returns:\n",
    "        requests.Response object\n",
    "    \"\"\"\n",
    "    payload = {\"inputs\": data}\n",
    "    return requests.post(ENDPOINT, json=payload)\n",
    "\n",
    "\n",
    "def predict_dataframe_split(df):\n",
    "    \"\"\"\n",
    "    Make predictions using the 'dataframe_split' format.\n",
    "    Includes column names separately from data.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        requests.Response object\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"dataframe_split\": {\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"data\": df.values.tolist()\n",
    "        }\n",
    "    }\n",
    "    return requests.post(ENDPOINT, json=payload)\n",
    "\n",
    "\n",
    "def predict_dataframe_records(df):\n",
    "    \"\"\"\n",
    "    Make predictions using the 'dataframe_records' format.\n",
    "    Each row is a dictionary with column names as keys.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        requests.Response object\n",
    "    \"\"\"\n",
    "    payload = {\"dataframe_records\": df.to_dict(orient=\"records\")}\n",
    "    return requests.post(ENDPOINT, json=payload)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined:\")\n",
    "print(\"  - check_server()\")\n",
    "print(\"  - predict_inputs(data)\")\n",
    "print(\"  - predict_dataframe_split(df)\")\n",
    "print(\"  - predict_dataframe_records(df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Server Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MLflow Model Serving Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[0] Checking model server...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if not check_server():\n",
    "    print(\"  ERROR: Model server is not running!\")\n",
    "    print(\"\\n  Start it with:\")\n",
    "    print(\"    mlflow models serve -m 'models:/iris-serving-model/Production' -p 5001 --no-conda\")\n",
    "    print(\"\\n  Then re-run this notebook.\")\n",
    "else:\n",
    "    print(f\"  Server is running at {MODEL_SERVER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test samples representing each class\n",
    "# These are typical values for each iris species\n",
    "samples = [\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Typical setosa\n",
    "    [6.2, 2.9, 4.3, 1.3],  # Typical versicolor\n",
    "    [7.7, 3.0, 6.1, 2.3],  # Typical virginica\n",
    "]\n",
    "\n",
    "# Create DataFrame version\n",
    "sample_df = pd.DataFrame(samples, columns=feature_names)\n",
    "\n",
    "print(\"\\n[Test Data]\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_df.to_string(index=False))\n",
    "print(\"\\nExpected classes: setosa, versicolor, virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Using 'inputs' Format (Simplest)\n",
    "\n",
    "The simplest format - just send a list of feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 1: 'inputs' format]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nPayload format:\")\n",
    "print('  {\"inputs\": [[5.1, 3.5, 1.4, 0.2], ...]}')\n",
    "\n",
    "response = predict_inputs(samples)\n",
    "\n",
    "print(f\"\\nStatus: {response.status_code}\")\n",
    "\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    # Handle different response formats\n",
    "    predictions = result.get(\"predictions\", result)\n",
    "    \n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"Classes: {[target_names[p] for p in predictions]}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Using 'dataframe_split' Format\n",
    "\n",
    "Includes column names and data separately. Good for preserving schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 2: 'dataframe_split' format]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nPayload format:\")\n",
    "print('''  {\n",
    "    \"dataframe_split\": {\n",
    "      \"columns\": [\"sepal length\", ...],\n",
    "      \"data\": [[5.1, 3.5, 1.4, 0.2], ...]\n",
    "    }\n",
    "  }''')\n",
    "\n",
    "response = predict_dataframe_split(sample_df)\n",
    "\n",
    "print(f\"\\nStatus: {response.status_code}\")\n",
    "\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    predictions = result.get(\"predictions\", result)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Using 'dataframe_records' Format\n",
    "\n",
    "Each row is a dictionary - most explicit and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 3: 'dataframe_records' format]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nPayload format:\")\n",
    "print('''  {\n",
    "    \"dataframe_records\": [\n",
    "      {\"sepal length\": 5.1, \"sepal width\": 3.5, ...},\n",
    "      ...\n",
    "    ]\n",
    "  }''')\n",
    "\n",
    "response = predict_dataframe_records(sample_df)\n",
    "\n",
    "print(f\"\\nStatus: {response.status_code}\")\n",
    "\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    predictions = result.get(\"predictions\", result)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 4: Single Prediction]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Single sample\n",
    "single_sample = [[5.1, 3.5, 1.4, 0.2]]\n",
    "\n",
    "response = predict_inputs(single_sample)\n",
    "\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    pred = result.get(\"predictions\", result)[0]\n",
    "    \n",
    "    print(f\"Input: {single_sample[0]}\")\n",
    "    print(f\"Prediction: {pred} ({target_names[pred]})\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Batch Prediction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 5: Batch Prediction (100 samples)]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate random samples within typical Iris ranges\n",
    "np.random.seed(42)\n",
    "batch = np.random.uniform(\n",
    "    low=[4.0, 2.0, 1.0, 0.1],    # Min values\n",
    "    high=[8.0, 4.5, 7.0, 2.5],   # Max values\n",
    "    size=(100, 4)                 # 100 samples, 4 features\n",
    ").tolist()\n",
    "\n",
    "# Time the request\n",
    "start = time.time()\n",
    "response = predict_inputs(batch)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    predictions = result.get(\"predictions\", result)\n",
    "    \n",
    "    print(f\"Samples: {len(batch)}\")\n",
    "    print(f\"Time: {elapsed:.3f}s\")\n",
    "    print(f\"Throughput: {len(batch)/elapsed:.1f} predictions/sec\")\n",
    "    \n",
    "    # Show distribution\n",
    "    dist = Counter(predictions)\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    for class_id, count in sorted(dist.items()):\n",
    "        print(f\"  {target_names[class_id]}: {count}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[Test 6: Error Handling]\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test 1: Wrong number of features\n",
    "print(\"\\nWrong number of features (3 instead of 4):\")\n",
    "response = predict_inputs([[1, 2, 3]])  # Only 3 features!\n",
    "print(f\"  Status: {response.status_code}\")\n",
    "if not response.ok:\n",
    "    print(f\"  Error (expected): {response.text[:100]}...\")\n",
    "\n",
    "# Test 2: Invalid data type\n",
    "print(\"\\nInvalid data type (strings instead of numbers):\")\n",
    "response = predict_inputs([[\"a\", \"b\", \"c\", \"d\"]])  # Strings!\n",
    "print(f\"  Status: {response.status_code}\")\n",
    "if not response.ok:\n",
    "    print(f\"  Error (expected): {response.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cURL Examples for Testing\n",
    "\n",
    "Here are curl commands you can use from the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"cURL Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "# Single prediction:\n",
    "curl -X POST {ENDPOINT} \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"inputs\": [[5.1, 3.5, 1.4, 0.2]]}}'\n",
    "\n",
    "# Multiple predictions:\n",
    "curl -X POST {ENDPOINT} \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"inputs\": [[5.1, 3.5, 1.4, 0.2], [6.2, 2.9, 4.3, 1.3]]}}'\n",
    "\n",
    "# DataFrame format (with column names):\n",
    "curl -X POST {ENDPOINT} \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"dataframe_split\": {{\n",
    "    \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n",
    "    \"data\": [[5.1, 3.5, 1.4, 0.2]]\n",
    "  }}}}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Input Formats\n",
    "\n",
    "| Format | When to Use | Example |\n",
    "|--------|------------|----------|\n",
    "| `inputs` | Simple arrays, no column names | `{\"inputs\": [[5.1, 3.5, ...]]}` |\n",
    "| `dataframe_split` | Need column names, efficient | `{\"dataframe_split\": {\"columns\": [...], \"data\": [...]}}` |\n",
    "| `dataframe_records` | Human-readable, explicit | `{\"dataframe_records\": [{\"col\": val, ...}]}` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use `inputs` for simplicity** - When you know the column order\n",
    "2. **Use `dataframe_split` for efficiency** - Best for large batches\n",
    "3. **Use `dataframe_records` for clarity** - Self-documenting requests\n",
    "4. **Always handle errors** - Check response status codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Model Serving Tests Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"  1. How to make REST API predictions\")\n",
    "print(\"  2. Three different input formats\")\n",
    "print(\"  3. How to handle batch predictions\")\n",
    "print(\"  4. How to debug common errors\")\n",
    "print(\"  5. cURL commands for terminal testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
