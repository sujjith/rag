apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-model-server
  namespace: mlflow
  labels:
    app: mlflow-model-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow-model-server
  template:
    metadata:
      labels:
        app: mlflow-model-server
    spec:
      containers:
        - name: model-server
          image: ghcr.io/mlflow/mlflow:v2.19.0
          ports:
            - containerPort: 5001
          command:
            - sh
            - -c
            - |
              echo "Model server ready. Waiting for model URI via environment..."
              echo "To serve a model, update MODEL_URI env var and restart the pod."
              # Default: serve a placeholder that returns 503
              python -c "
              from http.server import HTTPServer, BaseHTTPRequestHandler
              import json
              class Handler(BaseHTTPRequestHandler):
                  def do_GET(self):
                      if self.path == '/health':
                          self.send_response(200)
                          self.send_header('Content-Type', 'application/json')
                          self.end_headers()
                          self.wfile.write(json.dumps({'status': 'ok', 'model': 'none'}).encode())
                      else:
                          self.send_response(404)
                          self.end_headers()
                  def do_POST(self):
                      self.send_response(503)
                      self.send_header('Content-Type', 'application/json')
                      self.end_headers()
                      self.wfile.write(json.dumps({'error': 'No model loaded. Set MODEL_URI env var.'}).encode())
              HTTPServer(('0.0.0.0', 5001), Handler).serve_forever()
              "
          env:
            - name: MLFLOW_TRACKING_URI
              value: "http://mlflow-server:5000"
            - name: MODEL_URI
              value: ""  # Set this to models:/<name>/<stage> when ready
          volumeMounts:
            - name: artifacts-storage
              mountPath: /mlartifacts
              readOnly: true
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
      volumes:
        - name: artifacts-storage
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mlflow-model-server
  namespace: mlflow
spec:
  selector:
    app: mlflow-model-server
  ports:
    - port: 5001
      targetPort: 5001
      nodePort: 30501
  type: NodePort
